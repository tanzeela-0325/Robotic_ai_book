"use strict";(globalThis.webpackChunkai_humanoid_robotics_book=globalThis.webpackChunkai_humanoid_robotics_book||[]).push([[353],{7515(e,n,o){o.r(n),o.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"modules/module-4-vla/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Welcome to Module 4, where we learn how to create robots that can understand and respond to natural human commands.","source":"@site/docs/modules/module-4-vla/index.md","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/","permalink":"/Robotic_ai_book/docs/modules/module-4-vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/tanzeela-0325/Robotic_ai_book/edit/main/docs/modules/module-4-vla/index.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Module 4 - Vision-Language-Action (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Isaac Sim Architecture","permalink":"/Robotic_ai_book/docs/modules/module-3-ai-brain/chapter-2-isaac-sim-architecture"},"next":{"title":"Chapter 1: Multimodal Cognition in Robotics","permalink":"/Robotic_ai_book/docs/modules/module-4-vla/chapter-1-multimodal-cognition"}}');var t=o(4848),a=o(8453);const l={sidebar_label:"Module 4 - Vision-Language-Action (VLA)"},s="Module 4: Vision-Language-Action (VLA)",r={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.p,{children:"Welcome to Module 4, where we learn how to create robots that can understand and respond to natural human commands."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This module covers:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Vision-Language-Action models for robotics"}),"\n",(0,t.jsx)(n.li,{children:"Natural language understanding in robotics"}),"\n",(0,t.jsx)(n.li,{children:"Action planning from human commands"}),"\n",(0,t.jsx)(n.li,{children:"Multimodal perception systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will understand:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"How VLA models enable natural human-robot interaction"}),"\n",(0,t.jsx)(n.li,{children:"Best practices for multimodal perception"}),"\n",(0,t.jsx)(n.li,{children:"How to implement action planning from language"}),"\n",(0,t.jsx)(n.li,{children:"Advanced techniques for embodied AI systems"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453(e,n,o){o.d(n,{R:()=>l,x:()=>s});var i=o(6540);const t={},a=i.createContext(t);function l(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);